{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.arize.com/phoenix/retrieval/quickstart-retrieval\n",
    "#https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/qdrant_langchain_instrumentation_search_and_retrieval_tutorial.ipynb#scrollTo=sgZQUJ0LVxaQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "#import os\n",
    "#from getpass import getpass\n",
    "\n",
    "# Third-party library imports\n",
    "import nest_asyncio\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Miscellaneous imports\n",
    "\n",
    "# Configuration and Initialization\n",
    "nest_asyncio.apply()\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to make your LLM application observable, it must be instrumented. That is, the code must emit traces. The instrumented data must then be sent to an Observability backend, in our case the Phoenix server.\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8081\")\n",
    "weaviate_client = weaviate.connect_to_local(\"localhost\",\"8081\")#v4\n",
    "index_name=\"Phoenix_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.collections.delete_all()\n",
    "weaviate_client.collections.list_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "weaviate_client.collections.create(\n",
    "    index_name,\n",
    "    #see notes above re: the docker modules that need to be enabled for text2vec* to work correctly -e ENABLE_MODULES=text2vec-ollama\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama( \n",
    "        model=\"nomic-embed-text\",    \n",
    "        api_endpoint=\"http://host.docker.internal:11434\",\n",
    "    ),\n",
    "    # generative_config=Configure.Generative.ollama(\n",
    "    #     api_endpoint = \"http://host.docker.internal:11434\",\n",
    "    #     model=\"jonphi\"\n",
    "    # ),\n",
    "\n",
    "    # properties=[\n",
    "    #     Property(name=\"page_content\", data_type=DataType.TEXT),\n",
    "    #     Property(name=\"source\", data_type=DataType.INT),\n",
    "    # ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "model_name = \"nomic-embed-text\"\n",
    "embedding = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "#todo - look into keeping bullets together in available loader libraries\n",
    "#loader = TextLoader(\"/home/jonot480/Documents/Coke OCG.txt\")\n",
    "loader = TextLoader(\"/home/jonot480/Documents/paulgraham.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "#print(*docs, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.weaviate import Weaviate\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "weav = Weaviate.from_documents(\n",
    "    docs, \n",
    "    embedding=embedding,\n",
    "    client=client,    \n",
    "    index_name=index_name,\n",
    "    #prefer_grpc=True, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.ollama import ChatOllama\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "\n",
    "retriever = weav.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": num_retrieved_documents\n",
    "    }, \n",
    "    enable_limit=True\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-4-turbo-preview\"\n",
    "llm = ChatOllama(model_name=chat_model_name, temperature=0.0)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    metadata={\"application_type\": \"question_answering\"},\n",
    "    callbacks=[handler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain/langchain_query_dataframe_with_user_feedbackv2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    row = query_df.iloc[i]\n",
    "    response = chain.invoke(row[\"text\"])\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
