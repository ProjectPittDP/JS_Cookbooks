{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/v0.2/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoke tracing via phoenix\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8081\")\n",
    "weaviate_client = weaviate.connect_to_local(\"localhost\",\"8081\")#v4\n",
    "index_name=\"Phoenix_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.collections.delete_all()\n",
    "weaviate_client.collections.list_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Configure\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "weaviate_client.collections.create(\n",
    "    index_name,\n",
    "    #see notes above re: the docker modules that need to be enabled for text2vec* to work correctly -e ENABLE_MODULES=text2vec-ollama\n",
    "    vectorizer_config=Configure.Vectorizer.text2vec_ollama( \n",
    "        model=\"nomic-embed-text\",    \n",
    "        api_endpoint=\"http://host.docker.internal:11434\",\n",
    "    ),\n",
    "    # generative_config=Configure.Generative.ollama(\n",
    "    #     api_endpoint = \"http://host.docker.internal:11434\",\n",
    "    #     model=\"jonphi\"\n",
    "    # ),\n",
    "\n",
    "    # properties=[\n",
    "    #     Property(name=\"page_content\", data_type=DataType.TEXT),\n",
    "    #     Property(name=\"source\", data_type=DataType.INT),\n",
    "    # ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import pprint\n",
    "\n",
    "\n",
    "#todo - look into keeping bullets together in available loader libraries\n",
    "loader = TextLoader(\"/home/jonot480/Documents/paulgraham.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024,chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "#print(*docs, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.weaviate import Weaviate\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "vectordb = Weaviate.from_documents(\n",
    "    docs, \n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text'),\n",
    "    client=client,    \n",
    "    index_name=index_name,\n",
    "    #prefer_grpc=True, \n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = weaviate_client.collections.get(index_name)\n",
    "response = collection.query.fetch_objects()\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.properties)  # Inspect returned objects\n",
    "    #print(o.properties[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "question = \"What is the effective date?\"\n",
    "llm = ChatOllama(model=\"phi3\",temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "\n",
    "question = \"What is the effective date?\"\n",
    "\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "#langchain's code doesn't work (see section below, so i used this to get around it)\n",
    "#https://github.com/langchain-ai/langchain/issues/17352\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm( retriever=vectordb.as_retriever(), llm=llm, prompt=QUERY_PROMPT )\n",
    "\n",
    "unique_docs = retriever_from_llm.invoke(\"What is the effective date?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in unique_docs:\n",
    "    print(i)\n",
    "#print(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is langchain's code but it doesn't work \n",
    "#https://python.langchain.com/v0.2/docs/how_to/MultiQueryRetriever/\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectordb.as_retriever(), llm_chain=llm_chain\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.invoke(\"What does the course say about regression?\")\n",
    "len(unique_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
